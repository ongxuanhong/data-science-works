{"cells":[{"cell_type":"markdown","source":["# SparkSession\nA SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files.\nThe entry point to programming Spark with the Dataset and DataFrame API."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n.master(\"local[2]\") \\\n.appName(\"Word Count\") \\\n.config(\"spark.some.config.option\", \"some-value\") \\\n.getOrCreate()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["spark.conf.get(\"spark.some.config.option\") == spark.sparkContext.getConf().get(\"spark.some.config.option\") == \"some-value\"\nspark2 = SparkSession.builder.config(\"k2\", \"v2\").getOrCreate()\n\n# there is a valid global default SparkSession\nspark.conf.get(\"spark.some.config.option\") == spark2.conf.get(\"spark.some.config.option\")\nspark.conf.get(\"k2\") == spark2.conf.get(\"k2\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["# DataFrame\nA distributed collection of data grouped into named columns"],"metadata":{}},{"cell_type":"markdown","source":["## From list of tuples"],"metadata":{}},{"cell_type":"code","source":["l = [(\"Alice\", 1)]\nspark.createDataFrame(l).collect()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["spark.createDataFrame(l, [\"name\", \"age\"]).collect()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["d = [{\"name\": \"Alice\", \"age\": 1}]\nspark.createDataFrame(d).collect()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## From RDDs"],"metadata":{}},{"cell_type":"code","source":["rdd = sc.parallelize(l)\nspark.createDataFrame(rdd).collect()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# with list of column names\ndf = spark.createDataFrame(rdd, [\"name\", \"age\"])\ndf.collect()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# with Row definition\nfrom pyspark.sql import Row\nPerson = Row(\"name\", \"age\")\nperson = rdd.map(lambda r: Person(*r))\ndf2 = spark.createDataFrame(person)\ndf2.collect()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# with schema definition\nfrom pyspark.sql.types import *\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n  ])\ndf3 = spark.createDataFrame(rdd, schema)\ndf3.collect()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# with string definition, New in version 2.0.\nrdd = sc.parallelize(l)\nprint spark.createDataFrame(rdd, \"a: string, b: int\").collect()\nrdd = rdd.map(lambda row: row[1])\nprint spark.createDataFrame(rdd, \"int\").collect()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## From pandas"],"metadata":{}},{"cell_type":"code","source":["import pandas\nprint spark.createDataFrame(df.toPandas()).collect()\nprint spark.createDataFrame(pandas.DataFrame([[\"Alice\", 2]])).collect()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["# SQLContext"],"metadata":{}},{"cell_type":"code","source":["# New in version 2.0\ndf.createOrReplaceTempView(\"table1\")\ndf2 = spark.sql(\"SELECT name as N, age as A from table1\")\ndf2.collect()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Create DataFrame"],"metadata":{}},{"cell_type":"markdown","source":["## From list of tuples"],"metadata":{}},{"cell_type":"code","source":["l = [(\"Alice\", 1)]\nprint sqlContext.createDataFrame(l).collect()\nprint sqlContext.createDataFrame(l, [\"name\", \"age\"]).collect()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# not deprecated if use sqlContext instead of SparkSession\nd = [{\"name\": \"Alice\", \"age\": 1}]\nsqlContext.createDataFrame(d).collect()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## From RDDs"],"metadata":{}},{"cell_type":"code","source":["rdd = sc.parallelize(l)\nsqlContext.createDataFrame(rdd).collect()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["df = sqlContext.createDataFrame(rdd, [\"name\", \"age\"])\ndf.collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql import Row\nPerson = Row(\"name\", \"age\")\nperson = rdd.map(lambda r: Person(*r))\ndf2 = sqlContext.createDataFrame(person)\ndf2.collect()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n  ])\ndf3 = sqlContext.createDataFrame(rdd, schema)\ndf3.collect()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["print sqlContext.createDataFrame(df.toPandas()).collect()\nprint sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["print sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\nrdd = sc.parallelize(l)\nrdd = rdd.map(lambda row: row[1])\nprint sqlContext.createDataFrame(rdd, \"int\").collect()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["df.toDF(\"f1\", \"f2\", \"f3\").collect()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df.toJSON().first()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["list(df.toLocalIterator())"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["df.toPandas()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["## Temp table"],"metadata":{}},{"cell_type":"code","source":["sqlContext.registerDataFrameAsTable(df, \"table1\")\nsqlContext.registerDataFrameAsTable(df2, \"table2\")\nprint sqlContext.tableNames()\ndf3 = sqlContext.tables()\nprint df3\nprint df3.filter(\"tableName = 'table1'\").first()\nsqlContext.dropTempTable(\"table1\")\nsqlContext.dropTempTable(\"table2\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## UDF: User Defined Function"],"metadata":{}},{"cell_type":"code","source":["sqlContext.registerFunction(\"stringLengthString\", lambda x: len(x))\nsqlContext.sql(\"SELECT stringLengthString('test')\").collect()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nsqlContext.registerFunction(\"stringLengthInt\", lambda x: len(x), IntegerType())\nsqlContext.sql(\"SELECT stringLengthInt('test')\").collect()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["sqlContext.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\nsqlContext.sql(\"SELECT stringLengthInt('test')\").collect()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["# Working with DataFrame"],"metadata":{}},{"cell_type":"code","source":["l = [(\"Alice\", 2, 12), (\"Bob\", 5, 25)]\nrdd = sc.parallelize(l)\ndf = sqlContext.createDataFrame(rdd, \"name: string, age: int, height: int\")\ndf.collect()\n\ndf.createTempView(\"people\")\ndf2 = spark.sql(\"select * from people\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["df.repartition(10).rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["data = df.union(df).repartition(\"age\")\ndata.show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["data = data.repartition(7, \"age\")\ndata.show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["data.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["data = data.repartition(\"name\", \"age\")\ndata.show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# withColumn(colName, col)\n# Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\ndf.withColumn(\"age2\", df.age + 2).collect()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["df.withColumnRenamed(\"age\", \"age2\").collect()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["print df.select(df.age.cast(\"string\").alias(\"ages\")).collect()\nprint df.select(df.age.cast(StringType()).alias(\"ages\")).collect()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## Aggregate\nAggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg())."],"metadata":{}},{"cell_type":"code","source":["df.agg({\"age\": \"max\"}).collect()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.agg(F.min(df.age)).collect()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["gdf = df.groupBy(df.name)\nsorted(gdf.agg({\"*\": \"count\"}).collect())"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["from pyspark.sql import functions as F\nsorted(gdf.agg(F.min(df.age)).collect())"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["## Alias"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf_as1 = df.alias(\"df_as1\")\ndf_as2 = df.alias(\"df_as2\")\njoined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), \"inner\")\njoined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["## Stats"],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["df.schema"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["df.storageLevel"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["print df.groupBy().sum(\"age\").collect()\nprint df.groupBy().sum(\"age\", \"height\").collect()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["df.groupBy().avg(\"age\").collect()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["df.groupBy().avg(\"age\", \"height\").collect()"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["print df.name\nprint df[\"name\"]\nprint df.age + 1"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["# cube(*col): Create a multi-dimensional cube for the current DataFrame using the specified columns, so we can run aggregation on them.\ndf.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["df.describe([\"age\"]).show()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["df.distinct().count()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["df.dtypes"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["df.explain()"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["df.explain(True)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["df.groupBy().avg().collect()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["df.groupBy(\"name\").agg({\"age\": \"mean\"}).collect()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["df.groupBy(df.name).avg().collect()"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["df.groupBy([\"name\", df.age]).count().collect()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["print df.groupBy().max(\"age\").collect()\nprint df.groupBy().max(\"age\", \"height\").collect()"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["print df.groupBy().mean(\"age\").collect()\nprint df.groupBy().mean(\"age\", \"height\").collect()"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["## Join"],"metadata":{}},{"cell_type":"code","source":["print df.select(\"age\", \"name\").collect()\nprint df2.select(\"name\", \"height\").collect()\ndf.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", df2.height).collect()"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["df.drop(\"age\").collect()"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["df.drop(df.age).collect()"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["df.join(df2, df.name == df2.name, \"inner\").drop(df.name).drop(df.age).collect()"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["df.join(df2, \"name\", \"inner\").drop(\"age\", \"height\").collect()"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["from pyspark.sql import Row\ndf = sc.parallelize([\n    Row(name=\"Alice\", age=5, height=80),\n    Row(name=\"Alice\", age=5, height=80),\n    Row(name=\"Alice\", age=10, height=80)\n  ]).toDF()\ndf.dropDuplicates().show()"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["df.dropDuplicates([\"name\", \"height\"]).show()"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["df.join(df2, 'name', 'outer').select('name', df.height).collect()"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["cond = [df.name == df2.name, df.age == df2.age]\ndf.join(df2, cond, 'outer').select(df.name, df2.age).collect()"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["df.join(df2, 'name').select(df.name, df2.height).collect()"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["df.join(df2, ['name', 'age']).select(df.name, df.age).collect()"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["## Filter"],"metadata":{}},{"cell_type":"code","source":["l = [(\"Alice\", 2, 12), (\"Bob\", 5, 25)]\nrdd = sc.parallelize(l)\ndf = sqlContext.createDataFrame(rdd, \"name: string, age: int, height: int\")\n\nprint df.filter(df.age > 3).collect()\nprint df.filter(\"age > 3\").collect()\nprint df.where(\"age=2\").collect()"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["df.first()"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["def print_name(person):\n  print person.name\ndf.foreach(print_name)"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["def print_name(people):\n  for person in people:\n    print person.name\ndf.foreachPartition(print_name)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["df.head()"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["print df.limit(1).collect()\nprint df.limit(0).collect()"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["# orderBy\nprint df.sort(df.age.desc()).collect()\nprint df.sort(\"age\", ascending=False).collect()\nprint df.orderBy(df.age.desc()).collect()\n\nfrom pyspark.sql.functions import *\nprint df.sort(asc(\"age\")).collect()\nprint df.sort(desc(\"age\"), \"name\").collect()\nprint df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["print df.filter(df.name.endswith(\"ice\")).collect()\ndf.filter(df.name.endswith(\"ice$\")).collect()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["# get subfield RDD > RDD, gets a field by name in a StructField.\nfrom pyspark.sql import Row\ndf1 = sc.parallelize([Row(r=Row(a=1, b=\"b\"))]).toDF()\ndf1.select(df1.r.getField(\"b\")).show()\ndf1.select(df1.r.getField(\"a\")).show()"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["# RDD contains list and dictionary\ndf1 = sc.parallelize([([1, 2], {\"key\": \"value\"})]).toDF([\"l\", \"d\"])\ndf1.select(df1.l.getItem(0), df1.d.getItem(\"key\")).show()\ndf1.select(df1.l[0], df1.d[\"key\"]).show()"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["from pyspark.sql import Row\ndf1 = sc.parallelize([Row(name=u\"Tom\", height=80), Row(name=u\"Alice\", height=None)]).toDF()\nprint df1.filter(df1.height.isNotNull()).collect()\nprint df1.filter(df1.height.isNull()).collect()"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["print df[df.name.isin(\"Bob\", \"Mike\")].collect()\nprint df[df.age.isin(1, 2, 3)].collect()"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["df.filter(df.name.like(\"Al%\")).collect()"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"markdown","source":["# Working with Sample"],"metadata":{}},{"cell_type":"code","source":["df.na.replace([\"Alice\", \"Bob\"], [\"A\", \"B\"], \"name\").show()"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["# sample(withReplacement, fraction, seed=None)\ndf.sample(False, 0.5, 42).count()"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["# sampleBy(col, fractions, seed=None)\ndataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\nsampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\nsampled.groupBy(\"key\").count().orderBy(\"key\").show()"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["df.selectExpr(\"age * 2\", \"abs(age)\").collect()"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["# show(n=20, truncate=True)\n# truncate â€“ If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length truncate and align cells right.\ndf.show(truncate=3)"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":["# Working with Row"],"metadata":{}},{"cell_type":"code","source":["row = Row(name=\"Alice\", age=11)\nprint row\nprint row[\"name\"], row[\"age\"]\nprint row.name, row.age\nprint \"name\" in row\nprint \"wrong_key\" in row"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["# Row also can be used to create another Row like class, then it could be used to create Row objects\nPerson = Row(\"name\", \"age\")\nprint Person\nprint Person(\"Alice\", 11)"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["# asDict(recursive=False)\nprint Row(name=\"Alice\", age=11).asDict()\nrow = Row(key=1, value=Row(name=\"a\", age=2))\nprint row.asDict()\nprint row.asDict(True)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":119}],"metadata":{"name":"PySparkSQL","notebookId":2217893066036922},"nbformat":4,"nbformat_minor":0}
